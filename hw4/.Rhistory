library(e1071)
library(caret)
df =  read.csv("D:/Data Mining/hw4/naive_bayes_binary.csv",colClasses = c('factor','factor','factor','factor','factor','factor','factor','factor','factor','factor','factor'))
n = nrow(df)
train = df[1:(n/2),]
test = df[((n/2)+1):n,]
model = naiveBayes(V11~.,data=train)
model
pred = predict(model,test,type="class")
cm = confusionMatrix(pred,test$V11)
cm
d1=read.table("D:/Data Mining/hw4/student-mat.csv",sep=";",header=TRUE)
d2=read.table("D:/Data Mining/hw4/student-por.csv",sep=";",header=TRUE)
d3=merge(d1,d2,by=c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet"))
print(nrow(d3)) # 382 students
d1=read.table("D:/Data Mining/hw4/student-mat.csv",sep=";",header=TRUE)
d2=read.table("D:/Data Mining/hw4/student-por.csv",sep=";",header=TRUE)
d3=merge(d1,d2,by=c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet"))
print(nrow(d3)) # 382 students
View(d3)
View(d3)
ncol(d3)
d3=merge(d1,d2,by=c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet"),all=FALSE)
print(nrow(d3)) # 382 students
d3=merge(d1,d2,by=c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet"),no.dups=FALSE)
d3=merge(d1,d2,by=c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet"),all=TRUE)
d3=merge(d1,d2,by=c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet"),all.x =TRUE)
d3=merge(d1,d2,by=c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet"),all =TRUE)
d3=merge(d1,d2,by=c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet"))
X = scan("http://pages.iu.edu/∼mtrosset/StatInfeR/Data/sample771.dat")
# (a)
plot(ecdf(X))
# (b)
mean = mean(X)
variance = var(X)
# (c)
median = median(X)
IQR = quantile(X,0.75) - quantile(X,0.25)
# (d)
a = IQR/sqrt(variance)
# (e)
plt = boxplot(X)
# (f)
norm_plt = qqnorm(X)
# (g)
den_plt = plot(density(X))
# (h)
IQR_t = qnorm(0.75) - qnorm(0.25)
X = scan("http://pages.iu.edu/∼mtrosset/StatInfeR/Data/sample771.dat")
# (a)
plot(ecdf(X))
# (b)
mean = mean(X)
variance = var(X)
# (c)
median = median(X)
IQR = quantile(X,0.75) - quantile(X,0.25)
# (d)
a = IQR/sqrt(variance)
# (e)
plt = boxplot(X)
# (f)
norm_plt = qqnorm(X)
# (g)
den_plt = plot(density(X))
# (h)
IQR_t = qnorm(0.75) - qnorm(0.25)
# Ideally, for normal distributed, the ratio of IQR to standard deviation should be close to 1.35
# It is posible that the sample has been drawn from a normal distribution as the ratio of IQR to standard deviation is 1.41 which is close to 1.35
X1 = c(93,87,81,80,78,95,83,81,76,74,88,86,85,76,68)
X2 = c(90,83,71,67,63,100,98,97,68,64,92,84,79,74,68)
# (a)
boxplot(X1,X2,names=c("Al wins","NL wins"),ylab="wins")
plot(density(X1))
plot(density(X2))
IQR1 = quantile(X1,0.75) - quantile(X1,0.25)
IQR2 = quantile(X2,0.75) - quantile(X2,0.25)
plot(density(X1))
plot(density(X2))
sd1 = sqrt(var(X1))
sd2 = sqrt(var(X2))
a = IQR1/sd1
b = IQR2/sd2
qqnorm(X1)
qqnorm(X2)
norm_plt = qqnorm(X)
X = scan("http://pages.iu.edu/~mtrosset/StatInfeR/Data/sample774.dat")
X
X = scan("http://pages.iu.edu/~mtrosset/StatInfeR/Data/sample774.dat")
plot(ecdf(X))
mean = mean(X)
variance = var(X)
median = median(X)
IQR = quantile(X,0.75) - quantile(X,0.25)
ratio = IQR/sqrt(variance)
ratio = IQR/sqrt(variance)
plot(density(X))
Y = log(X)
qqnorm(Y)
plot(density(Y))
IQR1 = quantile(Y,0.75) - quantile(Y,0.25)
sd = sqrt(var(Y))
ratio1 = IQR1/sd
X = scan("http://pages.iu.edu/~mtrosset/StatInfeR/Data/test351.dat")
plot(density(X))
qqnorm(X)
sd1 = sqrt(var(X))
IQR = quantile(X,0.75) - quantile(X,0.25)
a = IQR/sd1
boxplot(X)
Q1 = quantile(X,0.25) - 1.5*IQR
Q2 = quantile(X,0.75) + 1.5*IQR
X
library(e1071)
library(caret)
df =  read.csv("D:/Data Mining/hw4/naive_bayes_binary.csv",colClasses = c('factor','factor','factor','factor','factor','factor','factor','factor','factor','factor','factor'))
n = nrow(df)
train = df[1:(n/2),]
test = df[((n/2)+1):n,]
model = naiveBayes(V11~.,data=train)
model
pred = predict(model,test,type="class")
cm = confusionMatrix(pred,test$V11)
cm
d1=read.table("D:/Data Mining/hw4/student-mat.csv",sep=";",header=TRUE)
d2=read.table("D:/Data Mining/hw4/student-por.csv",sep=";",header=TRUE)
d3=merge(d1,d2,by=c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet"))
print(nrow(d3)) # 382 students
View(d3)
View(d3)
d3$y = ifelse(d3$G3.x>10,1,0)
View(d3)
View(d3)
d3$y
head(d3)
head(d1)
d1$y = ifelse(d1$G3>10,1,0)
d1=read.table("D:/Data Mining/hw4/student-mat.csv",sep=";",header=TRUE)
d1$y = ifelse(d1$G3>10,1,0)
head(d1)
library(e1071)
library(caret)
df =  read.csv("D:/Data Mining/hw4/naive_bayes_binary.csv",colClasses = c('factor','factor','factor','factor','factor','factor','factor','factor','factor','factor','factor'))
n = nrow(df)
train = df[1:(n/2),]
test = df[((n/2)+1):n,]
model = naiveBayes(V11~.,data=train)
model
pred = predict(model,test,type="class")
cm = confusionMatrix(pred,test$V11)
cm
library(rpart)
d1=read.table("D:/Data Mining/hw4/student-mat.csv",sep=";",header=TRUE)
d1$y = ifelse(d1$G3>10,1,0)
View(d1)
View(d1)
View(d1)
df = subset(d1,select=-c(G1,G2,G3))
head(df)
n = nrow(df)
n = nrow(df)
n_train = round(0.75*n)
set.seed(100)
train_ind = sample(1:n,n_train)
train = df[train_ind,]
test = df[-train_ind,]
model = rpart(formula = y~.,data = train,method="class")
rpart.plot(model)
install.packages("rpart.plot")
rpart.plot(model)
library(rpart.plot)
rpart.plot(model)
rpart.plot(model)
model = rpart(formula = y~.,data = train,method="class",control=rpart.control(maxdepth=1))
rpart.plot(model)
rpart.plot(model)
model = rpart(formula = y~.,data = train,method="class",control=rpart.control(maxdepth=2))
rpart.plot(model)
model = rpart(formula = y~.,data = train,method="class",control=rpart.control(maxdepth=3))
rpart.plot(model)
model = rpart(formula = y~.,data = train,method="class",control=rpart.control(maxdepth=4))
rpart.plot(model)
model = rpart(formula = y~.,data = train,method="class",control=rpart.control(maxdepth=5))
rpart.plot(model)
model = rpart(formula = y~.,data = train,method="class",control=rpart.control(maxdepth=6))
rpart.plot(model)
model = rpart(formula = y~.,data = train,method="class",control=rpart.control(maxdepth=7))
rpart.plot(model)
model = rpart(formula = y~.,data = train,method="class",control=rpart.control(maxdepth=8))
rpart.plot(model)
model = rpart(formula = y~.,data = train,method="class",control=rpart.control(maxdepth=2))
rpart.plot(model)
model = rpart(formula = y~.,data = train,method="class",control=rpart.control(maxdepth=1))
rpart.plot(model)
model = rpart(formula = y~.,data = train,method="class",control=rpart.control(maxdepth=1))
train$pred = predict(model,train,type="class")
View(train)
View(train)
printcp(model)
model = rpart(formula = y~.,data = train,method="class",control=rpart.control(maxdepth=10))
printcp(model)
cp(model)
df = printcp(model)
model = rpart(formula = y~.,data = train,method="class",control=rpart.control(maxdepth=10))
printcp(model)
model = rpart(formula = y~.,data = train,method="class",control=rpart.control(maxdepth=12))
printcp(model)
plotcp(model)
?accuracy
model = rpart(formula = y~.,data = train,method="class",control=rpart.control(maxdepth=10))
train$pred = predict(model,train,type="class")
error = 1 - mean(train$y == train$pred)
error
g_error = vector(length=10)
g_error = vector(length=10)
for (i in 1:10)
model = rpart(formula = y~.,data = train,method="class",control=rpart.control(maxdepth=i))
train$pred = predict(model,train,type="class")
error = 1 - mean(train$y == train$pred)
g_error[i] = error+0.05*i
g_eroor
g_error
g_error = vector(length=10)
g_rror
g_error
g_error = vector(,10)
g_error[1] = 1
for (i in 1:10)
model = rpart(formula = y~.,data = train,method="class",control=rpart.control(maxdepth=i))
train$pred = predict(model,train,type="class")
error = 1 - mean(train$y == train$pred)
g_error[i] = error+0.01*i
g_error = vector(,10)
g_error = vector(,10)
error = vector(,10)
for (i in 1:10)
model = rpart(formula = y~.,data = train,method="class",control=rpart.control(maxdepth=i))
train$pred = predict(model,train,type="class")
error[i] = 1 - mean(train$y == train$pred)
g_error[i] = error+0.01*i
